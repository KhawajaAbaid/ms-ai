{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4e345ac-01b5-4a50-b783-7c9549c7a461",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56c7c14f-0e61-4282-8689-23dd73302f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import copy, deepcopy\n",
    "from typing import List, Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3916319f-2cee-45b2-a932-52884a2bbcef",
   "metadata": {},
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a5bec0-0b1c-4adc-8605-0a5412cf6358",
   "metadata": {},
   "source": [
    "### Question 1 - Draw a Decision Tree based on Boolean functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d676074f-e913-4ea9-980a-84c1dac3285f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, label):\n",
    "        self.label = label\n",
    "\n",
    "YES = Node(\"YES\")\n",
    "NO = Node(\"YES\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5db17f-bb26-429b-b3cb-dadc91b80b2e",
   "metadata": {},
   "source": [
    "1. A ∧ B ̄ ∧ C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bfa4b1c-0781-4afd-bcea-300e7f62eae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'q1_1.png'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = Digraph(format='png', graph_attr={'rankdir': 'TB'})\n",
    "A, B, C = Node('A'), Node('B'), Node('C')\n",
    "#no_1, no_2, no_3 = Node('No'), Node('No'), Node('No')\n",
    "nodes = [A, B, C]\n",
    "#for n in nodes:\n",
    "#    graph.node(name=str(id(n)), label=n.label, shape='record')\n",
    "edges = (\n",
    "    (A, 'True', B),\n",
    "    (A, 'False', Node('NO')),\n",
    "    (B, 'True', Node('NO')),\n",
    "    (B, 'False', C),\n",
    "    (C, 'True', Node('YES')),\n",
    "    (C, 'False', Node('NO')),\n",
    "    )\n",
    "\n",
    "for edge_info in edges:\n",
    "    n1, n2 = edge_info[0], edge_info[-1]\n",
    "    graph.node(name=str(id(n1)), label=n1.label, shape='record')\n",
    "    graph.node(name=str(id(n2)), label=n2.label, shape='record')\n",
    "    e_label = edge_info[1]\n",
    "    graph.edge(str(id(n1)), str(id(n2)), label=str(e_label))\n",
    "graph.render('q1_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fc9339-b8ea-45d1-8b89-e1e2107b8f46",
   "metadata": {},
   "source": [
    "2. X ∧ Y ̄ ∨ X ̄ ∧ Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0950584-4ab6-4046-b25c-8a78b0e8f415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'q1_2.png'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = Digraph(format='png', graph_attr={'rankdir': 'TB'})\n",
    "X, Y1, Y2 = Node('X'), Node('Y'), Node('Y')\n",
    "nodes = [X, Y1, Y2]\n",
    "edges = (\n",
    "    (X, 'True', Y1),\n",
    "    (Y1, 'True', Node('NO')),\n",
    "    (Y1, 'False', Node('YES')),\n",
    "    (X, 'False', Y2),\n",
    "    (Y2, 'True', Node('YES')),\n",
    "    (Y2, 'False', Node('NO')),\n",
    "    )\n",
    "\n",
    "for edge_info in edges:\n",
    "    n1, n2 = edge_info[0], edge_info[-1]\n",
    "    graph.node(name=str(id(n1)), label=n1.label, shape='record')\n",
    "    graph.node(name=str(id(n2)), label=n2.label, shape='record')\n",
    "    label = edge_info[1]\n",
    "    graph.edge(str(id(n1)), str(id(n2)), label=label)\n",
    "graph.render('q1_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b4fc8d-4af9-4410-9f5c-b788c8e6a31b",
   "metadata": {},
   "source": [
    "3. X ∧ Y ∧ Z ∨ X ∧ Y ̄ ∧ W ∨ X ̄ ∧ Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0faa30f5-73b8-43e6-ab87-671e55b0f369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'q1_3.png'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = Digraph(format='png', graph_attr={'rankdir': 'TB'})\n",
    "X, Y1, Y2, Z, W = Node('X'), Node('Y'), Node('Y'), Node('Z'), Node('W')\n",
    "nodes = [X, Y1, Y2]\n",
    "edges = (\n",
    "    (X, 'True', Y1),\n",
    "    (Y1, 'True', Z),\n",
    "    (Z, 'True', Node('YES')),\n",
    "    (Z, 'False', Node('NO')),\n",
    "    (Y1, 'False', W),\n",
    "    (W, 'True', Node('YES')),\n",
    "    (W, 'False', Node('NO')),\n",
    "    (X, 'False', Y2),\n",
    "    (Y2, 'True', Node('YES')),\n",
    "    (Y2, 'False', Node('NO')),\n",
    "    )\n",
    "\n",
    "for edge_info in edges:\n",
    "    n1, n2 = edge_info[0], edge_info[-1]\n",
    "    graph.node(name=str(id(n1)), label=n1.label, shape='record')\n",
    "    graph.node(name=str(id(n2)), label=n2.label, shape='record')\n",
    "    label = edge_info[1]\n",
    "    graph.edge(str(id(n1)), str(id(n2)), label=label)\n",
    "graph.render('q1_3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770b9ef9-cd38-4cee-a3b8-68d66ad57c7b",
   "metadata": {},
   "source": [
    "### Question 2 - Root node selection using Information Gain and Gini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3733cfab-0de9-4d82-aea8-2e794928a1d0",
   "metadata": {},
   "source": [
    "#### Preliminary steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18d8240-971b-4a24-8eec-a2386ee555ab",
   "metadata": {},
   "source": [
    "##### We first conver the pdf table to one we can use in programming to make our life easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fb40e68-db5f-428d-ac54-815e279c14c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78ff443c-31a5-4905-b382-518360436f9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reader = PdfReader(\"IDAI610_PS1_DecisionTree.pdf\")\n",
    "page = reader.pages[1]\n",
    "page_text = page.extract_text(0)\n",
    "table_text = page_text[119:119 + 454]\n",
    "table_text = table_text.replace('\\n', ' ').replace(\n",
    "    'Preferred foot', 'Preferred-foot').split(' ')\n",
    "features = table_text[:6]\n",
    "data = table_text[6:]\n",
    "#display(feature_names)\n",
    "#display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1784d0-5690-43db-9333-bc71030df9f6",
   "metadata": {},
   "source": [
    "##### Now it's time to get to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b6df321-fffb-417e-921a-e5630bc1d04a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Player</th>\n",
       "      <th>League</th>\n",
       "      <th>Position</th>\n",
       "      <th>Preferred-foot</th>\n",
       "      <th>Capped</th>\n",
       "      <th>Shortlisted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>SerieA</td>\n",
       "      <td>CF</td>\n",
       "      <td>Left</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>LaLiga</td>\n",
       "      <td>LW</td>\n",
       "      <td>Right</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C</td>\n",
       "      <td>LaLiga</td>\n",
       "      <td>CF</td>\n",
       "      <td>Right</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Player  League Position Preferred-foot Capped Shortlisted\n",
       "0      A  SerieA       CF           Left    yes        True\n",
       "1      B  LaLiga       LW          Right     no        True\n",
       "2      C  LaLiga       CF          Right    yes        True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_features = 6\n",
    "n_samples = ord('N') - ord('A') + 1\n",
    "\n",
    "data_dict = dict()\n",
    "for i, name in enumerate(features):\n",
    "    data_dict[name] = [data[i + n_features * j] for j in range(n_samples)]\n",
    "\n",
    "df = pd.DataFrame(data_dict)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81524d73-832f-4ac9-9055-791e929ff369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>League</th>\n",
       "      <th>Position</th>\n",
       "      <th>Preferred-foot</th>\n",
       "      <th>Capped</th>\n",
       "      <th>Shortlisted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SerieA</td>\n",
       "      <td>CF</td>\n",
       "      <td>Left</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LaLiga</td>\n",
       "      <td>LW</td>\n",
       "      <td>Right</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LaLiga</td>\n",
       "      <td>CF</td>\n",
       "      <td>Right</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   League Position Preferred-foot Capped Shortlisted\n",
       "0  SerieA       CF           Left    yes        True\n",
       "1  LaLiga       LW          Right     no        True\n",
       "2  LaLiga       CF          Right    yes        True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's actually delete the player column cause it is of no interest to us\n",
    "# and also take target feature out of features list\n",
    "df = df.iloc[:, 1:]\n",
    "features = features[1:]\n",
    "target_feature = features.pop()\n",
    "n_features -= 2\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6c5faa7-8320-4b9e-8aec-e4f548dc7800",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_entropy(\n",
    "    dataset: pd.DataFrame,\n",
    "    target_feature: str,\n",
    "    n_labels: int = 2,\n",
    "    log_base: str = '2',\n",
    ")->float:\n",
    "    \"\"\"\n",
    "    Computes entropy.\n",
    "\n",
    "    Args:\n",
    "        dataset: Dataset to compute entropy for.\n",
    "        target_feature: Name of the target feature.\n",
    "        n_labels: Number of unqiue labels in the target feature.\n",
    "        log_base: base to use for the log function. Defaults to 2.\n",
    "\n",
    "    Returns:\n",
    "        Entropy.\n",
    "    \"\"\"\n",
    "    n_samples = len(dataset)\n",
    "    counts = dataset[target_feature].value_counts().tolist()\n",
    "    counts = np.pad(counts, (0, n_labels - len(counts)))\n",
    "    probs = counts / n_samples + 1e-15\n",
    "    log_fn = np.log2 if log_base == '2' else np.log\n",
    "    entropy = -np.sum(probs * log_fn(probs))\n",
    "    return entropy\n",
    "\n",
    "\n",
    "def compute_information_gain(\n",
    "    dataset: pd.DataFrame, \n",
    "    features: List[str],\n",
    "    target_feature: str,\n",
    "    n_labels: int = 2,\n",
    "    log_base:str = '2',\n",
    ")->List[float]:\n",
    "    \"\"\"\n",
    "    Computes information gain.\n",
    "    \n",
    "    Args:\n",
    "        dataset: Dataset in the form of a pandas dataframe.\n",
    "        features: List of feature names excluding the target feature.\n",
    "        target_feature: Name of the target feature\n",
    "        n_labels: Number of unqiue labels in the target feature.\n",
    "        log_base: base to use for the log function. Defaults to 2.\n",
    "    \n",
    "    Returns:\n",
    "        A list of floats representing information gain per feature.\n",
    "        Has one-to-one correspondence with the features list passed.\n",
    "    \"\"\"\n",
    "    overall_entropy = compute_entropy(dataset, target_feature, log_base=log_base)\n",
    "\n",
    "    ig_per_feature = []\n",
    "    for feature in features:\n",
    "        remainder = 0.\n",
    "        for category in dataset[feature].unique():\n",
    "            partition = dataset[dataset[feature] == category]\n",
    "            entropy = compute_entropy(\n",
    "                partition,\n",
    "                target_feature,\n",
    "                n_labels=n_labels,\n",
    "                log_base=log_base\n",
    "            )\n",
    "            remainder += len(partition)/len(dataset) * entropy\n",
    "        ig_per_feature.append(overall_entropy - remainder)\n",
    "        \n",
    "    return ig_per_feature\n",
    "\n",
    "\n",
    "def compute_gini_impurity(\n",
    "    dataset: pd.DataFrame,\n",
    "    features: List[str],\n",
    "    target_feature: str,\n",
    "    n_labels: int = 2\n",
    ")->List[float]:\n",
    "    \"\"\"\n",
    "    Computes gini impurity.\n",
    "    \n",
    "    Args:\n",
    "        dataset: Dataset in the form of a pandas dataframe.\n",
    "        features: List of feature names excluding the target feature.\n",
    "        target_feature: Name of the target feature\n",
    "        n_labels: Number of unqiue labels in the target feature.\n",
    "    \n",
    "    Returns:\n",
    "        A list of floats representing gini impurity per feature.\n",
    "        Has one-to-one correspondence with the features list passed.\n",
    "    \"\"\"\n",
    "    gini_impurity_per_feature = []\n",
    "    for feature in features:\n",
    "        weighted_impurity = 0.\n",
    "        for category in dataset[feature].unique():\n",
    "            partition = dataset[dataset[feature] == category]\n",
    "            label_counts = partition[target_feature].value_counts().tolist()\n",
    "            label_counts = np.pad(label_counts, (0, n_labels - len(label_counts)))\n",
    "            probs = label_counts / len(partition)\n",
    "            gini_impurity = 1.0 - np.sum(np.square(probs))\n",
    "            weighted_impurity += len(partition)/len(dataset) * gini_impurity\n",
    "        gini_impurity_per_feature.append(weighted_impurity)\n",
    "    return gini_impurity_per_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5384eb-797b-43ec-bdf4-59ed47df5e27",
   "metadata": {},
   "source": [
    "##### Verifying the results (as a sanity check)\n",
    "Will use natural log instead of log2 in my own implemention as well JUST for the\n",
    "sake of comparison becuase sklean only computes info gain with natural log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e6f142a-25de-453a-b49d-f61be10579f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SKLEARN] Information Gain for each feature: [0.10316446 0.13110121 0.17871515 0.        ]\n",
      "[MY IMPLEMENTATION] Information Gain for each feature: [np.float64(0.10316445961386744), np.float64(0.1311012079924112), np.float64(0.17871515126363802), np.float64(0.0)]\n",
      "\n",
      "[Me]: Dear computer, my eyes hurt comparing floats, can you please verify if all the values are close enough? Thanks! \n",
      "[Computer]: They indeed are!\n"
     ]
    }
   ],
   "source": [
    "# ===================== \n",
    "# SCIKIT-LEARN\n",
    "# =====================\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import OrdinalEncoder, LabelEncoder\n",
    "X = df.iloc[:, :-1]\n",
    "ordinal_enc = OrdinalEncoder()\n",
    "X = ordinal_enc.fit_transform(X)\n",
    "y = df.iloc[:, -1]\n",
    "label_enc = LabelEncoder()\n",
    "y = label_enc.fit_transform(y)\n",
    "info_gain = mutual_info_classif(X, y, discrete_features=True)\n",
    "print(\"[SKLEARN] Information Gain for each feature:\", info_gain)\n",
    "\n",
    "# ======================\n",
    "# MY IMPLEMENTATION\n",
    "# ======================\n",
    "ig_per_feature = compute_information_gain(df, features, target_feature, log_base='e')\n",
    "print(\"[MY IMPLEMENTATION] Information Gain for each feature:\", ig_per_feature)\n",
    "\n",
    "# incase you may wonder if there's small deviation\n",
    "print(\"\\n[Me]: Dear computer, my eyes hurt comparing floats, \"\n",
    "      \"can you please verify if all the values are close enough? Thanks! \")\n",
    "print(\"[Computer]: They indeed are!\" if np.allclose(ig_per_feature, info_gain) else \"Nope!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25b5169-550a-4fae-86ed-721907290cd1",
   "metadata": {},
   "source": [
    "### Root node selection procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee13cb87-8fda-4704-aa79-66db9efa530f",
   "metadata": {},
   "source": [
    "#### Pseudo code\n",
    "    function select_root_node(dataest, features, target_feature, n_labels, criterion):\n",
    "        // criterion must be in [ig, gini]\n",
    "    \n",
    "        criterion_fn = information_gain if criterion is ig else compute_gini_impurity\n",
    "        \n",
    "        criterion_values = criterion_fn(dataset, features, target_feature, n_labels)\n",
    "        \n",
    "        if criterion is gini:\n",
    "            feature_index = argmin(criterion_values)\n",
    "        \n",
    "        else:\n",
    "            feature_index = argmax(criterion_values)\n",
    "        \n",
    "        return features[feature_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25bfc75-26dd-4260-bc93-36c663285809",
   "metadata": {},
   "source": [
    "#### Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c319e892-505e-4c5e-8a98-bd9ad8258ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_root_node(\n",
    "    dataset: pd.DataFrame,\n",
    "    features: List[str],\n",
    "    target_feature: str,\n",
    "    n_labels: int = 2,\n",
    "    criterion: str = 'gini'):\n",
    "    assert criterion.lower() in ['ig', 'gini']\n",
    "    criterion_fn = (\n",
    "        compute_gini_impurity if criterion == 'gini'\n",
    "        else compute_information_gain\n",
    "    )\n",
    "    criterion_values = criterion_fn(dataset, features, target_feature, n_labels)\n",
    "    if criterion == 'gini':\n",
    "        feature_index = np.argmin(criterion_values)\n",
    "    else:\n",
    "        feature_index = np.argmax(criterion_values)\n",
    "    return features[feature_index]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "615f15a2-6094-47f3-9922-02da786e25bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root node using gini as criterion: Preferred-foot\n",
      "Root node using ig as criterion: Preferred-foot\n"
     ]
    }
   ],
   "source": [
    "features = ['League', 'Position', 'Preferred-foot', 'Capped']\n",
    "target_feature = 'Shortlisted'\n",
    "\n",
    "for criterion in ['gini', 'ig']:\n",
    "    print(f\"Root node using {criterion} as criterion: \", end=\"\")\n",
    "    print(select_root_node(df, features, target_feature, criterion=criterion))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c1bb4a-134e-4094-a558-3f5e3cec5b35",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2403e37-9baf-4130-8995-473ca7be590c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((341, 31), (114, 31), (114, 31))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For this problem, we'll import the wdbc dataset\n",
    "\n",
    "wdbc_train_df = pd.read_csv(\"./wdbc/wdbc_train.csv\")\n",
    "wbdc_dev_df = pd.read_csv(\"./wdbc/wdbc_dev.csv\")\n",
    "wdbc_test_df = pd.read_csv(\"./wdbc/wdbc_test.csv\")\n",
    "\n",
    "wdbc_train_df.shape, wbdc_dev_df.shape, wdbc_test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3191fed-5c84-49bd-b47b-b1412bc62309",
   "metadata": {},
   "source": [
    "skipping data processing as it is not a concern for the sake of this assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0c89d1-8d65-4330-9eff-09a56cb210a4",
   "metadata": {},
   "source": [
    "## ID3 implemention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c942e66d-63e9-419d-b562-9afbd6489f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3acd0bc-b2ca-400e-a8f5-23e0ae190138",
   "metadata": {},
   "outputs": [],
   "source": [
    "NodeType = Enum('NodeType', ['ROOT', 'INTERMEDIATE', 'LEAF'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f3a8800-3727-42cd-9a92-2eb975880c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"\n",
    "    Node class specifically developed for the implemention of id3 algorithm \n",
    "    in this notebook.\n",
    "\n",
    "    Args:\n",
    "        name: Name or label of the node.\n",
    "        majority_label: Majority label in the population where the node exists.\n",
    "        parent: Parent of the node. For root it is None.\n",
    "        why: Why was the node created. Serves as the edge label from parent to\n",
    "            the child node.\n",
    "        node_type: What type of node it is? One of [ROOT, INTERMEDIATE, LEAF]\n",
    "        depth: Depth at which the node occurs.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str, \n",
    "        majority_label:str,\n",
    "        parent=None,\n",
    "        why: str = \"404\",\n",
    "        node_type=NodeType.INTERMEDIATE,\n",
    "        depth: int = 0,\n",
    "    ):\n",
    "        self.name = name\n",
    "        self.parent = parent\n",
    "        self.depth = depth\n",
    "        self.why = why\n",
    "        self.children = []\n",
    "        self.node_type = node_type\n",
    "        self.majority_label = majority_label\n",
    "        self.mapping = dict()\n",
    "        if parent:\n",
    "           self.parent.add_child(self) \n",
    "    \n",
    "    def add_child(self, child_node):\n",
    "        \"\"\"\n",
    "        Adds a child to the current node and updates self.mapping for inference.\n",
    "\n",
    "        Args:\n",
    "            child_node: Child node to be added to as a child.\n",
    "        \"\"\"\n",
    "        child_node.depth = self.depth + 1\n",
    "        self.mapping[child_node.why] = child_node\n",
    "        self.children.append(child_node)\n",
    "\n",
    "    def predict(self, x: dict):\n",
    "        \"\"\"\n",
    "        Recursively goes through all the nodes depending on the given instance \n",
    "        and returns the outcome/label.\n",
    "\n",
    "        Args:\n",
    "            x: A data instance. Must be dict or dict-like.\n",
    "\n",
    "        Returns:\n",
    "            Resulting label based on the instance.\n",
    "        \"\"\"\n",
    "        if self.node_type == NodeType.LEAF:\n",
    "            return self.name\n",
    "        value = x[self.name]\n",
    "        # Return majority label when an unkown value is encountered\n",
    "        if value not in self.mapping:\n",
    "            return self.majority_label\n",
    "        next_node = self.mapping[value]\n",
    "        return next_node.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709216eb-457e-41e6-bee7-5005692eb4c1",
   "metadata": {},
   "source": [
    "#### id3 implementation allowing both information gain and gini "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27c663e3-d98d-4d7a-a090-edab523ede37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def id3(\n",
    "    dataset: pd.DataFrame,\n",
    "    features: List[str],\n",
    "    target_feature: str,\n",
    "    parent_node: Node = None,\n",
    "    why:str = '404',\n",
    "    criterion:str = \"ig\",\n",
    ")->Node:\n",
    "    \"\"\"\n",
    "    Khawaja's implementation of ID3 algorithm based on the psuedocode in the\n",
    "    book/slides allowing both information gain and gini as criterions.\n",
    "    This implementation utilizes pandas in addition to numpy to make life \n",
    "    easier.\n",
    "    \n",
    "    Args:\n",
    "        dataset: Must be pandas dataframe.\n",
    "        features: List of names of features in the dataset.\n",
    "        target_feature: Name of target feature.\n",
    "        parent_node: Node that called this function. For the first call it would\n",
    "            be None.\n",
    "        why: why was the node created, or what led to the creation of node.\n",
    "            In simpler words, it's the edge label that connects parent node \n",
    "            with the child node\n",
    "        criterion: Criteria to use for node splitting. One of ['ig', 'gini']\n",
    "\n",
    "    Returns:\n",
    "        The root node of the tree.\n",
    "    \"\"\"\n",
    "    assert criterion in ['ig', 'gini']\n",
    "    features = copy(features) # creates copy to avoid changes inplace\n",
    "    label_counts = dataset[target_feature].value_counts().tolist()\n",
    "    majority_label = dataset[target_feature].unique().tolist()[np.argmax(label_counts)]\n",
    "    \n",
    "    if criterion == 'ig':\n",
    "        criterion_fn = compute_information_gain\n",
    "    elif criterion == 'gini':\n",
    "        criterion_fn = compute_gini_impurity\n",
    "    else:\n",
    "        raise ValueError(\"Invalid value for criterion.\")\n",
    "    \n",
    "    # Base case 1:\n",
    "    # If all the instances in dataset have the same target label then\n",
    "    # return a decision tree witha leaf node \n",
    "    if len(label_counts) == 1:\n",
    "        new_node = Node(\n",
    "            name=majority_label,\n",
    "            majority_label=majority_label,\n",
    "            parent=parent_node,\n",
    "            why=why,\n",
    "            node_type=NodeType.LEAF)\n",
    "        return parent_node if parent_node else new_node\n",
    "\n",
    "    # Base case 2:\n",
    "    # If the feature set being considered is empty, return a decision tree with\n",
    "    # leaf node with the majority label in dataset\n",
    "    elif len(features) == 0:\n",
    "        new_node = Node(\n",
    "            name=majority_label,\n",
    "            majority_label=majority_label,\n",
    "            parent=parent_node,\n",
    "            why=why,\n",
    "            node_type=NodeType.LEAF)\n",
    "        return parent_node if parent_node else new_node\n",
    "    \n",
    "    # Base case 3:\n",
    "    # If the dataset is empty then return a decision tree with a leaf node with\n",
    "    # the label of the majority label in the data partition of the immediate\n",
    "    # parent\n",
    "    elif len(dataset) == 0:\n",
    "        new_node = Node(\n",
    "            name=parent_node.majority_label,\n",
    "            majority_label=None,\n",
    "            parent=parent_node,\n",
    "            why=why,\n",
    "            node_type=NodeType.LEAF\n",
    "        )\n",
    "        return parent_node if parent_node else new_node\n",
    "\n",
    "    # Else (now the fun part starts)\n",
    "    criteria_vals = criterion_fn(\n",
    "        dataset,\n",
    "        features,\n",
    "        target_feature)\n",
    "    #print(f\"{criteria_vals=}\")\n",
    "    if criterion == \"gini\":\n",
    "        idx = np.argmin(criteria_vals)\n",
    "    else:\n",
    "        idx = np.argmax(criteria_vals)\n",
    "    #print(f\"{idx}\")\n",
    "    fbest = features[idx]\n",
    "    new_node = Node(\n",
    "        name=fbest,\n",
    "        majority_label=majority_label,\n",
    "        parent=parent_node,\n",
    "        why=why,\n",
    "        node_type=NodeType.INTERMEDIATE)\n",
    "    #print(\"fbest: \", fbest)\n",
    "    features.remove(fbest)\n",
    "    for category in dataset[fbest].unique():\n",
    "        partition = dataset[dataset[fbest] == category]\n",
    "        id3(partition, features, target_feature, parent_node=new_node,\n",
    "            why=category, criterion=criterion)\n",
    "    return parent_node if parent_node else new_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46042e86-7809-4778-b04b-ba76e38c8ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_feature = 'Diagnosis'\n",
    "features = list(set(wdbc_train_df.columns) - set([target_feature]))\n",
    "wdbc_tree = id3(wdbc_train_df, features, target_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb95fb8-5b0b-478e-9d0e-47038815b99f",
   "metadata": {},
   "source": [
    "#### Tree graph plotting using graphviz\n",
    "I'd like to mention that I did take a shot at developing my own function for tree visualization which worked for very small trees but failed at large tree. The code is available in the accompanying `idai610-ps1-brainstorm.ipynb` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "737c70ad-6a0a-4bde-a216-846276d83f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "def trace(root):\n",
    "    nodes, edges, edge_labels = set(), set(), list()\n",
    "    def build(v):\n",
    "        if v not in nodes:\n",
    "            nodes.add(v)\n",
    "            for child in v.children:\n",
    "                edges.add((v, child))\n",
    "                edge_labels.append(child.why)\n",
    "                build(child)\n",
    "    build(root)\n",
    "    return nodes, edges, edge_labels\n",
    "\n",
    "def draw_graph(root):\n",
    "    nodes, edges, edge_labels = trace(root)\n",
    "    graph = Digraph(format='png', graph_attr={'rankdir': 'TB'})\n",
    "\n",
    "    for n in nodes:\n",
    "        graph.node(name=str(id(n)), label=n.name, shape='record')\n",
    "    for (n1, n2), label in zip(edges, edge_labels):\n",
    "        graph.edge(str(id(n1)), str(id(n2)), label=str(label))\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8766be57-562a-4da0-86a2-390e070311a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tree = id3(df, features, target_feature, None, criterion=\"ig\")\n",
    "#draw_graph(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc6ed75-f950-4309-8390-2791c4298b11",
   "metadata": {},
   "source": [
    "#### Alright buckle up, let's implement pruining!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc8ce3bd-140b-4e5d-a22f-b9d58a9380eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2\n",
    "\n",
    "def prune_tree(\n",
    "    root: Node,\n",
    "    dataset: pd.DataFrame,\n",
    "    target_feature: str,\n",
    "    significance_level: float = 0.05,\n",
    "    max_pruning_depth: int = 1,\n",
    "):\n",
    "    \"\"\"\n",
    "    Assumes binary target feature.\n",
    "\n",
    "    PLEASE NOTE: This function prunes the given tree in-place!\n",
    "\n",
    "    Args:\n",
    "        root: Root node of the given decision tree.\n",
    "        dataset: Dataset used in building of the given decision tree. It must be\n",
    "            the exact same dataset.\n",
    "        target_feature: Name of the target feature.\n",
    "        significance_level: Significance level for the statistical test.\n",
    "        max_pruning_depth: The maximum depth to which the tree can be pruned.\n",
    "            Remember the root is at depth 0, then 1, 2, 3... and so on.\n",
    "    \"\"\"\n",
    "    if len(root.children) == 0:\n",
    "        return root\n",
    "    visited_nodes_idx = []\n",
    "    penultimate_nodes = []\n",
    "    def dig(root):\n",
    "        for child in root.children:\n",
    "            if child.node_type == NodeType.LEAF:\n",
    "                if child.parent not in penultimate_nodes:\n",
    "                    penultimate_nodes.append(child.parent)\n",
    "            else:\n",
    "                dig(child)\n",
    "    dig(root)\n",
    "    \n",
    "    # would obviously break if the target feature isn't binary\n",
    "    # Note: I'm following nomenclature used in the R&N book\n",
    "    p, n = dataset[target_feature].value_counts().to_list()\n",
    "    labels = dataset[target_feature].unique()\n",
    "    \n",
    "    # Let's begin pruning\n",
    "    while True:\n",
    "        node = penultimate_nodes.pop(0)\n",
    "        if node not in visited_nodes_idx:\n",
    "            visited_nodes_idx.append(str(id(node)))\n",
    "        else:\n",
    "            if len(penultimate_nodes) == 0:\n",
    "                break\n",
    "            continue\n",
    "        if node.parent == None:\n",
    "            if len(penultimate_nodes) == 0:\n",
    "                break\n",
    "            continue\n",
    "        delta = 0.\n",
    "        degrees_of_freedom = dataset[node.name].nunique() - 1\n",
    "        for category in dataset[node.name].unique():\n",
    "            partition = dataset[dataset[node.name]==category]\n",
    "            if partition[target_feature].nunique() == 1:\n",
    "                if partition[target_feature].unique()[0] == labels[0]:\n",
    "                    pk = partition[target_feature].value_counts().tolist()[0]\n",
    "                    nk = 0\n",
    "                else:\n",
    "                    nk = partition[target_feature].value_counts().tolist()[0]\n",
    "                    pk = 0\n",
    "            else:\n",
    "                pk, nk = partition[target_feature].value_counts().tolist()\n",
    "            pk_hat = (len(partition) / len(dataset)) * p\n",
    "            nk_hat = (len(partition) / len(dataset)) * n\n",
    "            delta += (pk - pk_hat)**2 / pk_hat + (nk - nk_hat)**2 / nk_hat\n",
    "        threshold = chi2.ppf(1.0 - significance_level, degrees_of_freedom)\n",
    "        if delta < threshold:\n",
    "            # prune it away\n",
    "            if node in node.parent.children:\n",
    "                node.parent.children.remove(node)\n",
    "            new_node = Node(\n",
    "                name=node.majority_label,\n",
    "                majority_label=node.majority_label,\n",
    "                parent=node.parent,\n",
    "                why=node.why,\n",
    "                node_type=NodeType.LEAF,\n",
    "            )\n",
    "            penultimate_nodes.append(node.parent)\n",
    "            for child_node in node.children:\n",
    "                del child_node\n",
    "            del node\n",
    "        \n",
    "        if len(penultimate_nodes) == 0:\n",
    "            break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "20fde639-71ba-41b0-a73a-2fc6f1f98e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_tree(wdbc_tree, wdbc_train_df, target_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0e60b8-c35c-4d05-9674-834d7a4a38aa",
   "metadata": {},
   "source": [
    "##### Please note that I'm not visualizing the tree before and after pruning in this notebook to keep it concise as visualization is not a requirement of the given problem. But I do visualize trees in my brainstorm notebook also included in the zip file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ae67bb-e473-4cb0-bc34-9ce914b93148",
   "metadata": {},
   "source": [
    "# Problem 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5345a071-af2b-4534-a8ff-d616808cb4c4",
   "metadata": {},
   "source": [
    "## Use your decision tree to develop a model for WDBC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb76024-8d9c-4972-9445-c70738d4ec2a",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aa748710-cdb0-4ac9-ba6b-89e4c6f3d5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "wdbc_train_df = pd.read_csv(\"./wdbc/wdbc_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b0536002-c1ec-4faf-8262-cfd95f3f6398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wdbc_train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6e79e341-49e7-4ae9-8f02-b9ce3121acb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_feature = 'Diagnosis'\n",
    "features = list(set(wdbc_train_df.columns) - set([target_feature]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7beeea07-4914-4940-8a98-860d20458544",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = id3(wdbc_train_df, features, target_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23239d28-42c2-4773-962e-6d066c2c5d67",
   "metadata": {},
   "source": [
    "### Function for prediction on batch using our tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "73de3f9f-d6ba-4a89-b672-72a9038cede3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_batch_predict(decision_tree, x_batch: pd.DataFrame)->np.array:\n",
    "    return np.asarray(list(map(decision_tree.predict, x_batch.to_dict('records'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3300605-a399-4c85-973d-dad0dc679326",
   "metadata": {},
   "source": [
    "### Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee0b379-dd04-4759-a603-366759d7fd6a",
   "metadata": {},
   "source": [
    "Tuning focuses on selecting the best node-splitting criterion (Gini or IG).\n",
    "\n",
    "My implemention also incorporates the selection of chi-squared pruning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a8f81a2-f6bd-4523-af30-e62b16588177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114, 31)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wdbc_dev_df = pd.read_csv(\"./wdbc/wdbc_dev.csv\")\n",
    "wdbc_dev_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a5272ef4-0536-47df-9603-5342b1269f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy, copy\n",
    "\n",
    "def binary_accuracy(\n",
    "    y_true: np.array,\n",
    "    y_pred: np.array\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes the binary accuracy using the categorical values (not probabilites)\n",
    "\n",
    "    Args:\n",
    "        y_true: numpy array of true values.\n",
    "        y_pred: numpy array of predicted values.\n",
    "\n",
    "    Returns:\n",
    "        Binary ccuracy (a float value).\n",
    "    \"\"\"\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "def tree_tuner(\n",
    "    tree_builder: callable,\n",
    "    train_dataset: pd.DataFrame,\n",
    "    dev_dataset: pd.DataFrame,\n",
    "    features: List[str],\n",
    "    target_feature: str,\n",
    "    objective_fn: callable = binary_accuracy,\n",
    "    objective_type: str = 'max', # minimize or maximize\n",
    "):\n",
    "    '''\n",
    "    tree tuner aims the find the best tree with the best config from a given\n",
    "    space of hyperparameters.\n",
    "\n",
    "    Args:\n",
    "        tree_builder: A function that builds the tree. For instance, id3.\n",
    "        train_dataset: training dataset. Must be pandas dataframe.\n",
    "        dev_dataset: Dataset for tuning hyperparameters.\n",
    "        features: List of names of features in the dataset.\n",
    "        target_feature: Name of the target feature.\n",
    "        objective_fn: Objective function that you're trying to optimize the tree\n",
    "            for.\n",
    "        objective_type: Whether to maximize the objective value or minimize it.\n",
    "            Must be in ['min', 'max'].\n",
    "\n",
    "    Returns:\n",
    "        A tuple of best performing tree (i.e the root node), best config as a\n",
    "        dict and best score value.\n",
    "    '''\n",
    "    assert objective_type in ['min', 'max']\n",
    "    \n",
    "    best_tree = None\n",
    "    curr_config = dict()\n",
    "    best_config = dict()\n",
    "    best_score = -np.inf if objective_type == 'max' else np.inf\n",
    "    trial = 0\n",
    "    # Obviously not the best way to do 'grid search' but hey it looks more\n",
    "    # intutive when there' just two hyperparameters with only like 2 values each\n",
    "    for criterion in ['gini', 'ig']:\n",
    "        for prune in [False, True]:\n",
    "            curr_config['criterion'] = criterion\n",
    "            curr_config['prune'] = prune\n",
    "            # train\n",
    "            tree = tree_builder(\n",
    "                train_dataset,\n",
    "                copy(features),\n",
    "                target_feature,\n",
    "                criterion=criterion\n",
    "            )\n",
    "            if prune:\n",
    "                # Remember prune_tree prunes the tree in place\n",
    "                prune_tree(\n",
    "                    tree,\n",
    "                    train_dataset,\n",
    "                    target_feature,\n",
    "                    max_pruning_depth=1\n",
    "                )\n",
    "            # evaluate on dev\n",
    "            y_pred = tree_batch_predict(tree, dev_dataset)\n",
    "            score = objective_fn(\n",
    "                dev_dataset[target_feature].values,\n",
    "                y_pred\n",
    "            )\n",
    "            if objective_type == 'max' and score > best_score:\n",
    "                best_tree = deepcopy(tree)\n",
    "                best_config = curr_config\n",
    "                best_score = score\n",
    "            print(f\"Trial: {trial} | Score: {score} | Best Score: {best_score}\", end=\"\")\n",
    "            print(f\" | Current Config: {curr_config} | Best Config: {best_config}\")\n",
    "            trial += 1\n",
    "    return best_tree, best_config, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dd6b6e91-b251-4417-8c44-0b9361caeac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial: 0 | Score: 0.9649122807017544 | Best Score: 0.9649122807017544 | Current Config: {'criterion': 'gini', 'prune': False} | Best Config: {'criterion': 'gini', 'prune': False}\n",
      "Trial: 1 | Score: 0.9649122807017544 | Best Score: 0.9649122807017544 | Current Config: {'criterion': 'gini', 'prune': True} | Best Config: {'criterion': 'gini', 'prune': True}\n",
      "Trial: 2 | Score: 0.9649122807017544 | Best Score: 0.9649122807017544 | Current Config: {'criterion': 'ig', 'prune': False} | Best Config: {'criterion': 'ig', 'prune': False}\n",
      "Trial: 3 | Score: 0.9649122807017544 | Best Score: 0.9649122807017544 | Current Config: {'criterion': 'ig', 'prune': True} | Best Config: {'criterion': 'ig', 'prune': True}\n"
     ]
    }
   ],
   "source": [
    "target_feature = 'Diagnosis'\n",
    "features = list(set(wdbc_dev_df.columns) - set([target_feature]))\n",
    "\n",
    "best_tree, best_config, best_score = tree_tuner(\n",
    "    id3,\n",
    "    wdbc_train_df,\n",
    "    wdbc_dev_df,\n",
    "    features,\n",
    "    target_feature\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a412ae0-7b9f-4d8b-9f9c-809240c009bc",
   "metadata": {},
   "source": [
    "Developer Note: It is just weird that the results here are exactly the same.\n",
    "\n",
    "Please refer to the brainstorm notebook where I demonstrate both by visualizing and computing metrics that the pruning works and also gini and ig do result in different results sometimes (though for small they tend to produce exactly the same results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7df2b14-aea2-4a63-93cf-186fd6a02266",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "86614183-4d59-4c3c-b957-cc75d6889371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114, 31)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(\"./wdbc/wdbc_test.csv\")\n",
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2cdfc15d-ed73-4947-b5de-378436afe877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8947368421052632\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = np.mean(test_df[target_feature] == tree_batch_predict(best_tree, test_df))\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bab1e9-cac9-4dca-abea-37f507568d07",
   "metadata": {},
   "source": [
    "Not bad, eh?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2becf7d4-744b-482e-bd2b-ee5894ce8728",
   "metadata": {},
   "source": [
    "# [Optional] Discretize features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a1c0d6-e595-4708-b186-12e8e870a664",
   "metadata": {},
   "source": [
    "I'm using a very simple approach of binning the continuous features into categorical, specifically using the equal-width approach which works by discretizing continuous data by dividing the range of possible values into a specified number of intervals (bins) of equal size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "266cd0c0-5e74-4b38-be43-09792fb7b682",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_features_equal_width(dataset, n_bins=5):\n",
    "    \"\"\"\n",
    "    Bins continuous values into categorical.\n",
    "    \n",
    "    Args:\n",
    "        X: pandas dataset with continuous features\n",
    "        n_bins: Number of bins to use for categorization\n",
    "    \n",
    "    Returns:\n",
    "        A 2d numpy array of categroized features.\n",
    "    \"\"\"\n",
    "    dataset_arr = dataset.values\n",
    "    n_samples, n_features = X.shape\n",
    "    dataset_categorical = np.zeros_like(dataset_arr, dtype=int)\n",
    "    \n",
    "    for feature in range(n_features):\n",
    "        feature_values = dataset_categorical[:, feature]\n",
    "        bins = np.linspace(feature_values.min(), feature_values.max(), n_bins + 1)\n",
    "        \n",
    "        dataset_categorical[:, feature] = np.digitize(feature_values, bins[1:-1])\n",
    "    \n",
    "    return dataset_categorical"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
